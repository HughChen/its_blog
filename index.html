<!DOCTYPE html>
<html lang="en">
<title>A fast and exact game-theoretic algorithm to explain trees</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="stylesheet" type="text/css" href="index.css">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script language="JavaScript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/3.2.1/math.js"></script>
<script src="https://d3js.org/d3.v4.min.js"></script>
<script>d3v4 = d3;</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min.js"></script>
<script src="https://distill.pub/template.v1.js"></script>
<style id="distill-prerendered-styles" type="text/css">/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

html {
  font-size: 14px;
  line-height: 1.6em;
  /* font-family: "Libre Franklin", "Helvetica Neue", sans-serif; */
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  /*, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";*/
  text-size-adjust: 100%;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}

@media(min-width: 768px) {
  html {
    font-size: 16px;
  }
}

body {
  margin: 0;
}

a {
  color: #004276;
}

figure {
  margin: 0;
}

table {
  border-collapse: collapse;
  border-spacing: 0;
}

table th {
  text-align: left;
}

table thead {
  border-bottom: 1px solid rgba(0, 0, 0, 0.05);
}

table thead th {
  padding-bottom: 0.5em;
}

table tbody :first-child td {
  padding-top: 0.5em;
}

pre {
  overflow: auto;
  max-width: 100%;
}

p {
  margin-top: 0;
  margin-bottom: 1em;
}

sup, sub {
  vertical-align: baseline;
  position: relative;
  top: -0.4em;
  line-height: 1em;
}

sub {
  top: 0.4em;
}

.kicker,
.marker {
  font-size: 15px;
  font-weight: 600;
  color: rgba(0, 0, 0, 0.5);
}


/* Headline */

@media(min-width: 1024px) {
  d-title h1 span {
    display: block;
  }
}

/* Figure */

figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

figcaption+figure {

}

figure img {
  width: 100%;
}

figure svg text,
figure svg tspan {
}

figcaption,
.figcaption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

@media(min-width: 1024px) {
figcaption,
.figcaption {
    font-size: 13px;
  }
}

figure.external img {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

figcaption b,
figcaption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

@supports not (display: grid) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    display: block;
    padding: 8px;
  }
}

.base-grid,
distill-header,
d-title,
d-abstract,
d-article,
d-appendix,
distill-appendix,
d-byline,
d-footnote-list,
d-citation-list,
distill-footer {
  display: grid;
  justify-items: stretch;
  grid-template-columns: [screen-start] 8px [page-start kicker-start text-start gutter-start middle-start] 1fr 1fr 1fr 1fr 1fr 1fr 1fr 1fr [text-end page-end gutter-end kicker-end middle-end] 8px [screen-end];
  grid-column-gap: 8px;
}

.grid {
  display: grid;
  grid-column-gap: 8px;
}

@media(min-width: 768px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }
}

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 50px [middle-start] 50px [text-start kicker-end] 50px 50px 50px 50px 50px 50px 50px 50px [text-end gutter-start] 50px [middle-end] 50px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}




.base-grid {
  grid-column: screen;
}

/* .l-body,
d-article > *  {
  grid-column: text;
}

.l-page,
d-title > *,
d-figure {
  grid-column: page;
} */

.l-gutter {
  grid-column: gutter;
}

.l-text,
.l-body {
  grid-column: text;
}

.l-page {
  grid-column: page;
}

.l-body-outset {
  grid-column: middle;
}

.l-page-outset {
  grid-column: page;
}

.l-screen {
  grid-column: screen;
}

.l-screen-inset {
  grid-column: screen;
  padding-left: 16px;
  padding-left: 16px;
}


/* Aside */

d-article aside {
  grid-column: gutter;
  font-size: 12px;
  line-height: 1.6em;
  color: rgba(0, 0, 0, 0.6)
}

@media(min-width: 768px) {
  aside {
    grid-column: gutter;
  }

  .side {
    grid-column: gutter;
  }
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-title {
  padding: 2rem 0 1.5rem;
  contain: layout style;
  overflow-x: hidden;
}

@media(min-width: 768px) {
  d-title {
    padding: 4rem 0 1.5rem;
  }
}

d-title h1 {
  grid-column: text;
  font-size: 40px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

@media(min-width: 768px) {
  d-title h1 {
    font-size: 50px;
  }
}

d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  grid-column: text;
}

d-title .status {
  margin-top: 0px;
  font-size: 12px;
  color: #009688;
  opacity: 0.8;
  grid-column: kicker;
}

d-title .status span {
  line-height: 1;
  display: inline-block;
  padding: 6px 0;
  border-bottom: 1px solid #80cbc4;
  font-size: 11px;
  text-transform: uppercase;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-byline {
  contain: style;
  overflow: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  font-size: 0.8rem;
  line-height: 1.8em;
  padding: 1.5rem 0;
  min-height: 1.8em;
}


d-byline .byline {
  grid-template-columns: 1fr 1fr;
  grid-column: text;
}

@media(min-width: 768px) {
  d-byline .byline {
    grid-template-columns: 1fr 1fr 1fr 1fr;
  }
}

d-byline .authors-affiliations {
  grid-column-end: span 2;
  grid-template-columns: 1fr 1fr;
  margin-bottom: 1em;
}

@media(min-width: 768px) {
  d-byline .authors-affiliations {
    margin-bottom: 0;
  }
}

d-byline h3 {
  font-size: 0.6rem;
  font-weight: 400;
  color: rgba(0, 0, 0, 0.5);
  margin: 0;
  text-transform: uppercase;
}

d-byline p {
  margin: 0;
}

d-byline a,
d-article d-byline a {
  color: rgba(0, 0, 0, 0.8);
  text-decoration: none;
  border-bottom: none;
}

d-article d-byline a:hover {
  text-decoration: underline;
  border-bottom: none;
}

d-byline p.author {
  font-weight: 500;
}

d-byline .affiliations {

}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-article {
  contain: layout style;
  overflow-x: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  padding-top: 2rem;
  color: rgba(0, 0, 0, 0.8);
}

d-article > * {
  grid-column: text;
}

@media(min-width: 768px) {
  d-article {
    font-size: 16px;
  }
}

@media(min-width: 1024px) {
  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
}


/* H2 */


d-article .marker {
  text-decoration: none;
  border: none;
  counter-reset: section;
  grid-column: kicker;
  line-height: 1.7em;
}

d-article .marker:hover {
  border: none;
}

d-article .marker span {
  padding: 0 3px 4px;
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  position: relative;
  top: 4px;
}

d-article .marker:hover span {
  color: rgba(0, 0, 0, 0.7);
  border-bottom: 1px solid rgba(0, 0, 0, 0.7);
}

d-article h2 {
  font-weight: 600;
  font-size: 24px;
  line-height: 1.25em;
  margin: 2rem 0 1.5rem 0;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding-bottom: 1rem;
}

@media(min-width: 1024px) {
  d-article h2 {
    font-size: 36px;
  }
}

/* H3 */

d-article h3 {
  font-weight: 700;
  font-size: 18px;
  line-height: 1.4em;
  margin-bottom: 1em;
  margin-top: 2em;
}

@media(min-width: 1024px) {
  d-article h3 {
    font-size: 20px;
  }
}

/* H4 */

d-article h4 {
  font-weight: 600;
  text-transform: uppercase;
  font-size: 14px;
  line-height: 1.4em;
}

d-article a {
  color: inherit;
}

d-article p,
d-article ul,
d-article ol,
d-article blockquote {
  margin-top: 0;
  margin-bottom: 1em;
  margin-left: 0;
  margin-right: 0;
}

d-article blockquote {
  border-left: 2px solid rgba(0, 0, 0, 0.2);
  padding-left: 2em;
  font-style: italic;
  color: rgba(0, 0, 0, 0.6);
}

d-article a {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  text-decoration: none;
}

d-article a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.8);
}

d-article .link {
  text-decoration: underline;
  cursor: pointer;
}

d-article ul,
d-article ol {
  padding-left: 24px;
}

d-article li {
  margin-bottom: 1em;
  margin-left: 0;
  padding-left: 0;
}

d-article li:last-child {
  margin-bottom: 0;
}

d-article pre {
  font-size: 14px;
  margin-bottom: 20px;
}

d-article hr {
  grid-column: screen;
  width: 100%;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  margin-top: 60px;
  margin-bottom: 60px;
}

d-article section {
  margin-top: 60px;
  margin-bottom: 60px;
}

d-article span.equation-mimic {
  font-family: georgia;
  font-size: 115%;
  font-style: italic;
}

d-article > d-code,
d-article section > d-code  {
  display: block;
}

d-article > d-math[block],
d-article section > d-math[block]  {
  display: block;
}

@media (max-width: 768px) {
  d-article > d-code,
  d-article section > d-code,
  d-article > d-math[block],
  d-article section > d-math[block] {
      overflow-x: scroll;
      -ms-overflow-style: none;  // IE 10+
      overflow: -moz-scrollbars-none;  // Firefox
  }

  d-article > d-code::-webkit-scrollbar,
  d-article section > d-code::-webkit-scrollbar,
  d-article > d-math[block]::-webkit-scrollbar,
  d-article section > d-math[block]::-webkit-scrollbar {
    display: none;  // Safari and Chrome
  }
}

d-article .citation {
  color: #668;
  cursor: pointer;
}

d-include {
  width: auto;
  display: block;
}

d-figure {
  contain: layout style;
}

/* KaTeX */

.katex, .katex-prerendered {
  contain: style;
  display: inline-block;
}

/* Tables */

d-article table {
  border-collapse: collapse;
  margin-bottom: 1.5rem;
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
}

d-article table th {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
}

d-article table td {
  border-bottom: 1px solid rgba(0, 0, 0, 0.05);
}

d-article table tr:last-of-type td {
  border-bottom: none;
}

d-article table th,
d-article table td {
  font-size: 15px;
  padding: 2px 8px;
}

d-article table tbody :first-child td {
  padding-top: 2px;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

span.katex-display {
  text-align: left;
  padding: 8px 0 8px 0;
  margin: 0.5em 0 0.5em 1em;
}

span.katex {
  -webkit-font-smoothing: antialiased;
  color: rgba(0, 0, 0, 0.8);
  font-size: 1.18em;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

@media print {

  @page {
    size: 8in 11in;
    @bottom-right {
      content: counter(page) " of " counter(pages);
    }
  }

  html {
    /* no general margins -- CSS Grid takes care of those */
  }

  p, code {
    page-break-inside: avoid;
  }

  h2, h3 {
    page-break-after: avoid;
  }

  d-header {
    visibility: hidden;
  }

  d-footer {
    display: none!important;
  }

}
</style>

<style>
hover a {
    color: "Olive";
}

.no-margin {
	margin:0px;
	font-family:courier;
	font-size:16px;
}

.responsive {
	width: 100%;
	height: auto;
}

.bar {
    fill: #4DAF51;
}

.bar:hover {
	fill: #019788;
}

.axis {
    font-size: 14px;
}

.axis path,
.axis line {
    fill: none;
    display: none;
    shape-rendering: crispEdges;
}

.label {
	color: #4DAF51;
    font-size: 14px;
}

.d3-tip:after {
  box-sizing: border-box;
  display: inline;
  font-size: 10px;
  width: 100%;
  line-height: 1;
  color: rgba(0, 0, 0, 0.8);
  content: "\25BC";
  position: absolute;
  text-align: center;
}

.d3-tip.n:after {
  margin: -1px 0 0 0;
  top: 100%;
  left: 0;
}

#table_container {
	/*margin: 0 auto;*/
	background-color : #F4F6F6;
}

.slidecontainer {
  width: 100%;
}

.slider {
  -webkit-appearance: none;
  width: 100%;
  height: 15px;
  border-radius: 5px;
  background: #d3d3d3;
  outline: none;
  opacity: 0.7;
  -webkit-transition: .2s;
  transition: opacity .2s;
}

.slider:hover {
  opacity: 1;
}

.slider::-webkit-slider-thumb {
  -webkit-appearance: none;
  appearance: none;
  width: 25px;
  height: 25px;
  border-radius: 50%;
  background: #4CAF50;
  cursor: pointer;
}

.slider::-moz-range-thumb {
  width: 25px;
  height: 25px;
  border-radius: 50%;
  background: #4CAF50;
  cursor: pointer;
}

.node circle {
	fill: #fff;
	stroke: #ccc;
	stroke-width: 4px;
}

.node text { font: 12px sans-serif; }

.link {
	fill: none;
	stroke-width: 3px;
}

div.tooltip {
	position: absolute;
}

.container {
	width: 760px;
}

.first {
	width: 380px;
	float: left;
	height: 630px;
}

.second {
	width: 380px;
	float: left;
	height: 630px;
}

figcaption,
.figcaption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

@media(min-width: 1024px) {
figcaption,
.figcaption {
    font-size: 13px;
  }
}

figure.external img {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

figcaption b,
figcaption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

a.figure-number,
a.section-number {
    border-bottom-color: hsla(206, 90%, 20%, 0.3);
    text-transform: uppercase;
    font-size: .85em;
    color: hsla(206, 90%, 20%, 0.7);
}
a.figure-number::before {
    content: "Figure ";
}
a.figure-number:hover,
a.section-number:hover {
    border-bottom-color: hsla(206, 90%, 20%, 0.6);
}

.svg-container {
  display: inline-block;
  position: relative;
  width: 100%;
  padding-bottom: 100%; /* aspect ratio */
  vertical-align: top;
  overflow: hidden;
}
.svg-content-responsive {
  display: inline-block;
  position: absolute;
  top: 10px;
  left: 0;
}

svg .rect {
  fill: gold;
  stroke: steelblue;
  stroke-width: 5px;
}

.no-space {
  margin-top:0px;
  margin-bottom:0px;
  /*line-height:0.1em;*/
  /*padding-top:0.3em;*/
  padding-bottom:0.0em;
}

pre {
  background: #f5f2f0;
}
</style>

<head>
  <link href="prism.css" rel="stylesheet" />
</head>


<body>

  <d-front-matter>
    <code style="display: none;" type="text/json"
      >{ "title": "A fast and exact game-theoretic algorithm to explain trees", "description": "Shapley values have recently been adapted to explain machine learning algorithms; however, calculating them is NP-hard.  By restricting our machine learning model class to trees, we can calculate them exactly in linear time.",
      "authors": [
        { "author": "Hugh Chen", "authorURL": "http://hughchen.github.io/", "affiliation": "Paul G. Allen School of CSE", "affiliationURL": "https://www.cs.washington.edu/"  },
        { "author": "Scott Lundberg", "authorURL": "https://scottlundberg.com/", "affiliation": "Microsoft Research", "affiliationURL": "https://www.microsoft.com/en-us/research/"  },
        { "author": "Su-In Lee", "authorURL": "https://suinlee.cs.washington.edu/", "affiliation": "Paul G. Allen School of CSE", "affiliationURL": "https://www.cs.washington.edu/"  }
      ] }</code>
  </d-front-matter>

  <d-title>
    <h1 id="top">A fast and exact game-theoretic algorithm to explain trees</h1>
      <p style="font-size: 150%;">Shapley values have recently been adapted to explain machine learning algorithms; however, calculating them is NP-hard.  By restricting our machine learning model class to trees, we can calculate them exactly in linear time.</p>
      <img
        src="./images/trees.jpg"
        style='grid-column: text; width: 100%; padding-top: 20px; padding-bottom:20px;' />
  </d-title>
  <d-byline></d-byline>
  <d-article>

    <d-contents>
      <nav class="l-text toc figcaption">
        <a href="#top"><h3>Top</h3></a>
        <!--<div><a href="#introduction">Introduction</a></div>-->
        <!-- <div><a href="#introduction">Introduction</a></div> -->
        <div><a href="#background">Defining the feature attribution problem</a></div>
        <ul>
          <li><a href="#shapley_values">Back to basics with Shapley values</a></li>
          <li><a href="#shap_values">Adapting Shapley values to explain machine learning models</a></li>
          <li><a href="#masking_missing">Masking missing features</a></li>
          <li><a href="#shap_conditional_distribution">Imputing missing features by conditioning</a></li>
          <li><a href="#background_distribution">Single reference SHAP values</a></li>
        </ul>
        <div><a href="#algorithm">Explaining trees quickly and exactly</a></div>
        <ul>
          <li><a href="#define_tree_samples">Choose your own adventure</a></li>
          <li><a href="#brute_force">The brute force algorithm</a></li>
          <li><a href="#naive_implementation">The naive algorithm</a></li>
          <li><a href="#dynamic_programming_implementation">The dynamic algorithm</a></li>
          <li><a href="#final_considerations">Final considerations</a></li>
        </ul>
        <div><a href="#related_work">Related Work</a></div>
        <div><a href="#references">References</a></div>
      </nav>
    </d-contents>

  <!-- Introduction -->
  <div style="max-width:800px" id="abstract">
    <!-- <h2 id="introduction">Introduction</h2> -->
    <p>
      Nowadays, machine learning (ML) is widespread.  In this field, one of the most popular machine learning model types is tree-based models.  As evidence, a recent survey of data scientists and researchers found that tree models were both the second and third most popular class of method, beaten only by Logistic Regression (Figure 1).  Although small tree models can be interpretable <d-cite bibtex-key="rudin2019stop"></d-cite>, most tree models are generally large and hard for humans to interpret.  
    </p>

  <!-- Figure 1 -->
  <figure>
    <div id="fig1"></div>
    <figcaption>
      <a href="#fig1" class="figure-number">1</a>: The most popular data science methods according to a <a href="https://www.kaggle.com/surveys/2017">2017 Kaggle survey</a> (based on a total of 7,301 responses).
    </figcaption>
  </figure>

    <p>
    In order to explain these models, we use Shapley values - a unique game-theoretic solution for spreading credit between features.  First, we discuss SHAP values, an extension of Shapley values to machine learning models (<a href="#background">Section 1</a>).  However, exactly computing SHAP values for an arbitrary model is NP-hard <d-cite bibtex-key="matsui2001np"></d-cite>, but by focusing on explaining tree models, it is possible to compute them exactly in linear time (<a href="#algorithm">Section 2</a>).
    </p>

    <!-- <h3 id="motivation">Motivation</h3> -->
    <p>
    <i>What is the goal of this article?</i> 

    <p>
    Given that there is a long history of model explanations going awry when users do not understand what an explanation means (e.g., p-values for linear models <d-cite bibtex-key="schervish1996p"></d-cite>), it is critical to have a broadly accessible explanation of SHAP values and how they are obtained for tree models to ensure that they are not misused.  To this end, we aim to provide an easy to understand answer to two questions: 1.) What are SHAP values?  2.) How can we obtain SHAP values for trees?  In particular, we focus on explaining an algorithm to explain trees in the popular SHAP package called Interventional Tree Explainer (the algorithm was first described and empirically evaluated in <d-cite bibtex-key="lundberg2020local"></d-cite>).
  </p>
  </div>

  <hr>

  <!-- The Background Section -->
  <div style="max-width:800px">
    <h2 id="background">1. Defining the feature attribution problem</h2>
    
    <p>
 	   In this section, we describe Shapley values and a few versions that have been used to explain machine learning models.  Then, we use an example with a linear model to justify a specific extension of the Shapley values.  With this formulation, we show how obtaining the SHAP values reduces to an average of "single reference SHAP values" that can be thought of as explanations with respect to a single foreground sample (explicand, or sample being explained) and a single background sample (baseline, or sample the explicand is compared to).
	</p>


    <!-- What are Shapley Values? -->
    <h3 id="shapley_values">1.1 Back to basics with Shapley values</h3>
	<p>
    	Shapley values are a method to spread credit among players in a "coalitional game".  We can define the players to be a set \(N=\{1,\cdots,d\}\).  Then, the coalitional game is a function that maps subsets of the players to a scalar value:

    	$$
    	v(S):\text{Power Set}(N)\to\mathbb{R}^1
    	$$
    </p>

    <p>
    	To make these concepts more concrete, we can imagine a company that makes a profit \(v(S)\) that is determined by what combination of individuals they employ \(S\).  Furthermore, let's assume we know \(v(S)\) for all possible combinations of employees.  Then, the Shapley values assign credit to an individual \(i\) by taking a weighted average of how much the profit increases when \(i\) works with a group \(S\) versus when he does not work with \(S\).  Repeating this for all possible subsets \(S\) gives us the Shapley Values:
    	<!-- (include a simple graphic here?) -->
    	$$
    	\overbrace{\phi_i(v)}^{\text{Shapley value of }i}=\sum_{S\subseteq N\setminus\{i\}}\underbrace{\frac{|S|!(|N|-|S|-1)!}{|N|!}}_{\text{Weight }W(|S|,|N|)}(\overbrace{v(S\cup\{i\})-v(S)}^{\text{Profit individual }i\text{ adds}})
    	$$
    </p>

  <!-- Figure 2 -->
	<figure id="shapley_value_ex">
  
    <figcaption>
      <a href="#shapley_value_ex" class="figure-number">2</a>: Shapley values for a company that makes a profit \(v(S)\) based on it's three prospective employees \(Ava\), \(Ben\), and \(Cat\).
    </figcaption>

    <div style="margin-bottom:10px;">
  	  <table align="center" style="width:500px;border:none;margin-bottom:10px;">
  	  	<col width="200px"/>
      	<col width="250px"/>
      	<col width="50px"/>
        <tr align="center">
          <td style="border:none;" colspan="3">Coalitional game</td>
        </tr>
    		<tr>
    			<td>Subset \(S\)</td>
    			<td>Profit \(v(S)\)</td>
    			<td></td>
    		</tr>
    		<tr>
    			<td>\(\{\}\)</td>
    			<td><input type="range" min="-15" max="15" value="0" class="slider" id="s_n" onchange="calcSV()"></td>
    			<td><span id="s_n_out"></span></td>
    		</tr>
    		<tr>
    			<td>\(\{Ava\}\)</td>
    			<td><input type="range" min="-15" max="15" value="0" class="slider" id="s_a" onchange="calcSV()"></td>
    			<td><span id="s_a_out"></span></td>
    		</tr>
    		<tr>
    			<td>\(\{Ben\}\)</td>
    			<td><input type="range" min="-15" max="15" value="0" class="slider" id="s_b" onchange="calcSV()"></td>
    			<td><span id="s_b_out"></span></td>
    		</tr>
    		<tr>
    			<td>\(\{Cat\}\)</td>
    			<td><input type="range" min="-15" max="15" value="0" class="slider" id="s_c" onchange="calcSV()"></td>
    			<td><span id="s_c_out"></span></td>
    		</tr>
    		<tr>
    			<td>\(\{Ava,Ben\}\)</td>
    			<td><input type="range" min="-15" max="15" value="0" class="slider" id="s_ab" onchange="calcSV()"></td>
    			<td><span id="s_ab_out"></span></td>
    		</tr>
    		<tr>
    			<td>\(\{Ava,Cat\}\)</td>
    			<td><input type="range" min="-15" max="15" value="0" class="slider" id="s_ac" onchange="calcSV()"></td>
    			<td><span id="s_ac_out"></span></td>
    		</tr>
    		<tr>
    			<td>\(\{Ben,Cat\}\)</td>
    			<td><input type="range" min="-15" max="15" value="0" class="slider" id="s_bc" onchange="calcSV()"></td>
    			<td><span id="s_bc_out"></span></td>
    		</tr>
    		<tr>
    			<td>\(\{Ava,Ben,Cat\}\)</td>
    			<td><input type="range" min="-15" max="15" value="0" class="slider" id="s_abc" onchange="calcSV()"></td>
    			<td><span id="s_abc_out"></span></td>
    		</tr>
  	  </table>
    </div>

    <div style="margin-bottom:0px">
  	  <table align="center" style="border:none;">
		<col width="80px"/>
      	<col width="80px"/>
		<col width="80px"/>
      	<col width="80px"/>
		<col width="80px"/>
      	<col width="80px"/>
        <tr align="center">
          <td style="border:none;" colspan="6">Shapley values</td>
        </tr>
    		<tr>
    			<td align=center>\(\phi_{Ava}(v)\)</td>
    			<td align=center id="phi_a">0</td>
    			<td align=center>\(\phi_{Ben}(v)\)</td>
    			<td align=center id="phi_b">0</td>
    			<td align=center>\(\phi_{Cat}(v)\)</td>
    			<td align=center id="phi_c">0</td>
    		</tr>
  	  </table>
    </div>

    <div align="center" style="margin-bottom: 10px">
  	  <input type="button" value="Preset A" class="w3-button w3-teal" id="ex1_presetA" onclick="ex1_presetA()"> 
  	  <input type="button" value="Preset B" class="w3-button w3-green" id="ex1_presetB" onclick="ex1_presetB()"> 
  	  <input type="button" value="Preset C" class="w3-button w3-green" id="ex1_presetC" onclick="ex1_presetC()"> 
  	  <input type="button" value="Preset D" class="w3-button w3-green" id="ex1_presetD" onclick="ex1_presetD()"> 
    </div>

	  <p align="center" style="font-size:13px" id="ex1_preset_text">No credit.</p>

	</figure>


    <p>
      Shapley values are an excellent way to give credit to individuals in a coalitional game.  In fact, they are known to be a unique solution to spreading credit as defined by several desirable properties <d-cite bibtex-key="young1985monotonic"></d-cite> (all of which hold for any coalitional game, as can be seen in <a href="#shapley_value_ex">Figure 2</a>):
      <ul>
        <li>Local Accuracy/Efficiency: The sum of Shapley values for all employees adds up to the profit with all employees minus the profit with no employees:</li>
          $$
            \sum_{i\in N} \phi_i(v)=v(N)-v(\{\})
          $$
        <li>Consistency/Monotonicity: If an employee \(i\) always increases company \(v_1\)'s profit more than they would company \(v_2\) for all possible teams of employees, then \(i\)'s attribution for \(v_1\) should be greater than or equal to their attribution in \(v_2\):</li>
          $$
          v_1(S\cup {i})-v_1(S)\geq v_2(S\cup {i})-v_2(S) \forall S \implies \phi_i(v_1)\geq \phi_i(v_2)
          $$
        <li>Missingness: Employees \(i\) that don't help or hurt the company's profit must have no attribution.</li>
          $$
          v(S\cup {i})=v(S)\forall S\implies \phi_i(v)=0
          $$
      </ul>
    </p>

    <!-- What are SHAP Values? -->
    <h3 id="shap_values">1.2 Adapting Shapley values to explain machine learning models</h3>


    <p>
      Although Shapley values are an optimal solution for allocating credit among players in coalitional games, our goal is actually to allocate credit among features (\(x:=\{x_1,\cdots,x_d\}\in\mathbb{R}^d\)) in a machine learning model (\(f(x)\)).  This begs the question: how exactly do coalitional games differ to machine learning models?
    </p>

    <p>
      Well, a machine learning model with binary features \(x_i\in(0,1)\) <d-footnote>Where 1 indicates presence of a feature and 0 indicates absence of a feature.</d-footnote> would in fact be a coalitional game.  In this setting, the best way to assign credit to features is to just use the Shapley values where we define a new set function where features are 1 or 0 depending on if they are in the set:
        $$
        v(S):=f(z^S)\text{, where } z^S_i = 1\text{ if }i\in S\text{, }0\text{ otherwise}
        $$
      However, most machine learning models actually have continuous features.  In order to explain a machine learning model with continuous features, we now have to define both the presence and absence of features<d-footnote>More accurately, we have to define a new set function \(v(S)\) related to the model's prediction \(f(x)\) where the features in \(S\) are present and the remaining features are absent.</d-footnote>.
    </p>

    <p>
      First, to address the presence of a feature, our goal is to obtain <i>local feature attributions</i><d-footnote>As opposed to a <i>global feature attribution</i> that aims to assign per-feature importance for the model as a whole.</d-footnote>: a vector of importance for each feature of a model prediction for a specific sample \(x^f\) (explicand <d-footnote>The sample being explained.</d-footnote>).  Then, if feature \(i\) is present, we can simply set that feature to be from the explicand \(x_i^f\). The next step is to address the absence of a feature which can be done in many ways.
    </p>

  <h3 id="masking_missing">1.3 Masking missing features</h3>

    <p>
      One straightforward way to address the absence of a feature is to define a baseline \(x^b\).  In this case, if feature \(i\) is absent, we can simply set that feature to be \(x_i^b\).  Then the new set function is:
      $$
      v(S)=f(h^S)\text{, where } h^S_i = x^f_i\text{ if }i\in S\text{, }x^b_i\text{ otherwise}
      $$
    </p>

    <p>
      This specific adaptation of Shapley values to machine learning models is called Baseline Shapley, and were shown to be a unique solution to attribution methods with a single baseline based on cost-sharing literature <d-cite bibtex-key="sundararajan2019many"></d-cite>.  However, the choice of the baseline is a complicated one, and many different baselines have been considered including an all-zeros baseline, an average across features, a uniformly distributed baseline, and more (<d-cite bibtex-key="sundararajan2017axiomatic"></d-cite>, <d-cite bibtex-key="shrikumar2017learning"></d-cite>, <d-cite bibtex-key="fong2017interpretable"></d-cite>, <d-cite bibtex-key="sturmfels2020visualizing"></d-cite>) <d-footnote>For an in-depth discussion of this concept and a comparison of several baselines for Aumann-Shapley values applied to explaining images see <d-cite bibtex-key="sturmfels2020visualizing"></d-cite>.</d-footnote>.
    </p>


    <figure id="single_baselines_bad">

      <p align=center style="font-size: 15px"></p>

      <div align=center style="margin-bottom: -10px">
        <table style="border:none;">
          <col width="130px">
          <col width="40px">
          <col width="60px">
          <col width="40px">
          <col width="60px">
          <col width="40px">
          <col width="60px">
          <tr align="left">
          	<td colspan="7" style="border:none;">a.) Model and sample being explained.</td>
          </tr>
          <tr align="center">
          	<td align=right>Linear Model:</td>
            <td>\(\beta_1\)</td>
            <td>2</td>
            <td>\(\beta_2\)</td>
            <td>-1</td>
            <td>\(\beta_3\)</td>
            <td>10</td>
          </tr>
          <tr align="center">
          	<td align=right>Explicand:</td>
            <!-- Height (in) -->
            <td>\(x^f_1\)</td>
            <td>70</td>
            <!-- Weight (lbs) -->
			<td>\(x^f_2\)</td>
            <td>135</td>
            <!-- Gender 1-male, 0-female -->
            <td>\(x^f_3\)</td>
            <td>0</td>
          </tr>
        </table>
    </div>

      <hr style="margin-top:0px;margin-bottom:10px;">

      <div align=center style="margin-bottom: -10px">
        <table style="border:none;">
          <col width="130px">
          <col width="40px">
          <col width="60px">
          <col width="40px">
          <col width="60px">
          <col width="40px">
          <col width="60px">
          <tr align="left">
          	<td colspan="7" style="border:none;">b.) Zero baseline.</td>
          </tr>
          <tr align="center">
          	<td align=right>Baseline:</td>
            <td>\(x^b_1\)</td>
            <td>0</td>
            <td>\(x^b_2\)</td>
            <td>0</td>
            <td>\(x^b_3\)</td>
            <td>0</td>
          </tr>
          <tr align="center">
          	<td align=right>Attribution:</td>
            <td>\(\phi_1\)</td>
            <td>140</td>
            <td>\(\phi_2\)</td>
            <td>-135</td>
            <td>\(\phi_3\)</td>
            <td>0</td>
          </tr>
        </table>
      </div>


      <hr style="margin-top:0px;margin-bottom:10px;">

      <div align=center style="margin-bottom: -10px">
        <table style="border:none;">
          <col width="130px">
          <col width="40px">
          <col width="60px">
          <col width="40px">
          <col width="60px">
          <col width="40px">
          <col width="60px">
          <tr align="left">
          	<td colspan="7" style="border:none;">c.) Average baseline.</td>
          </tr>
          <tr align="center">
          	<td align=right>Baseline:</td>
            <td>\(x^b_1\)</td>
            <td>70</td>
            <td>\(x^b_2\)</td>
            <td>135</td>
            <td>\(x^b_3\)</td>
            <td>0.5</td>
          </tr>
          <tr align="center">
          	<td align=right>Attribution:</td>
            <td>\(\phi_1\)</td>
            <td>0</td>
            <td>\(\phi_2\)</td>
            <td>0</td>
            <td>\(\phi_3\)</td>
            <td>-5</td>
          </tr>
        </table>
      </div>

    <figcaption>
      <a href="#single_baselines_bad" class="figure-number">3</a>: Figure illustrating the downsides of a single baseline for Shapley values.  Feature 1 corresponds to age (inches), feature 2 is weight (pounds), and feature 3 is gender (0 is male, 1 is female).  The first baseline is an all-zero baseline.  The second is an average feature value baseline.
    </figcaption>
    </figure>

  <!-- Proof -->
  <div">
    <p><a onclick="hideshow('proof_linearmodel_baseline')"><strong>+ Technical details</strong></a></p>

    <div id="proof_linearmodel_baseline", style="display:none">
      <p>
        For masking, the SHAP values for a linear model are easy to compute:
        $$
        \begin{aligned}
        \phi_i(f,x^f,x^b)&= \sum_{S\in C } W(|S|,|N|)(f(h^{S\cup i}) {-} f(h^{S}))\\
        &= \sum_{S\in C } W(|S|,|N|)(\beta h^{S\cup i} {-} \beta h^{S})\\
        &=\sum_{S\in C } W(|S|,|N|) \beta_i (x^f_i-x^b_i)\\ 
        &= \beta_i (x^f_i-x^b_i)\\ 
        \end{aligned}
        $$
        Note that \(\sum_{S\in C } W(|S|,|N|)=1\).
    </p>
  </div>


    <p>
      One criticism of Baseline Shapley is that the choice of baseline is very influential to the resulting feature attributions.  In <a href="#single_baselines_bad" class="figure-number">3</a>, we can see that the choice of baseline heavily influences the resulting attribution value.
    </p>

    <p>
      <a href="#single_baselines_bad" class="figure-number">3b</a> is the all-zeros baseline.  In this case, it appears that height and weight are quite important, however being male is not important at all.  This is a bit counter-intuitive, because relative to being female, being male reduces the prediction for this individual.  Put another way, we can consider a new model where \(x_3\) is 0 for female, rather than 1 for female.  Then, an equivalent model would be \(y=2x_1-x_2-10x_3+10\).  In this case, using the same zero-baseline gives an attribution value of -10 instead of 0 for being male for the exact same model.  Since the meaning of zero is often arbitrary for different variables, selecting it as your baseline can result in misleading attributions.
    </p>

    <p>
      Alternatively, we could use a mean-baseline as in <a href="#single_baselines_bad" class="figure-number">3c</a>.  Although the mean of a binary variable does not have a natural interpretation it fixes the Gender problem.  However, for Height and Weight, we can see that the all-mean baseline introduces bias and effectively forces zero attribution to individuals with average heights and weights.  This is undesirable because their height and weight do indeed influence the model's prediction.
    </p>

    <p>
      To avoid the bias of using a single <i>background sample</i> (baseline) we can instead use many different baselines.  In the next section we will describe two approaches to incorporate <i>background distributions</i> to our set functions \(v(S)\) for a much better definition of missingness.
    </p>

    <h3 id="shap_conditional_distribution">1.4 Imputing missing features by conditioning</h3>

    <p>
		One natural approach to incorporate a background distribution to the set function is with a conditional expectation.  Instead of simply replacing "missing" features with a fixed value, we condition on the set of features that are "present" as if we know them and use those to guess at the "missing" features.  If we define \(D\) to be the background (underlying) distribution our samples are drawn from, the value of the game is:
    	$$
		  v(S)=\mathbb{E}_D[f(x)|x_{S}]
    	$$
    </p>

    <p>
    	One caveat is that getting this conditional expectation for actual data is very difficult.  Furthermore, even if you do manage to do so, the resulting explanations can end up having undesirable characteristics (more on this later).  Because our goal is just to explain the model itself, an arguably more natural approach is to use causal inference's <i>interventional conditional expectation</i>:
    	$$
    	v(S)=\mathbb{E}_D[f(x)|\text{do}(x_{S})]
    	$$
    	The <i>do</i> notation is causal inference's <i>do</i>-operator <d-cite bibtex-key="pearl2009causality"></d-cite>.  In words, we break the dependence between the features in \(S\) and the remaining features, which is analogous to <i>intervening</i> on the remaining features.  The motivation behind this decision comes from <d-cite bibtex-key="janzing2019feature"></d-cite> which is also very close to Random Baseline Shapley in <d-cite bibtex-key="sundararajan2019many"></d-cite>.  Additionally, this is exactly the assumption made by <a href="https://shap.readthedocs.io/en/latest/#shap.KernelExplainer">Kernel Explainer</a> <d-cite bibtex-key="lundberg2017unified"></d-cite> and <a href="https://shap.readthedocs.io/en/latest/#shap.SamplingExplainer">Sampling Explainer</a> from the SHAP package.
    </p>

    <!-- Figure 3 -->
    <figure id="linear_shap_ex">
      <div align=center style="margin-bottom: 20px">

    		<table style="width:110px;display:inline;margin-right:25px;border:none;">
  	      <col width="50px">
  	      <col width="65px">
  	      <tr align="center">
  	      	<td style="border:none;" colspan="2">Linear</td>
  	  	  </tr>
  	      <tr align="center">
  	      	<td colspan="2">Model</td>
  	  	  </tr>
    		  <tr align="center">
    		    <td>\(\beta_1\)</td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_b1" value="1"></td>
    		  </tr>
    		  <tr align="center">
    		    <td>\(\beta_2\)</td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_b2" value="2"></td>
    		  </tr>
    		  <tr align="center">
    		    <td>\(\beta_3\)</td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_b3" value="3"></td>
    		  </tr>
    		  <tr align="center">
    		    <td>\(\beta_4\)</td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_b4" value="4"></td>
    		  </tr>
    		</table>

    		<table style="width:300px;display:inline;margin-right:25px;border:none;">
  	      <col width="50px">
  	      <col width="65px">
  	      <col width="65px">
  	      <col width="65px">
  	      <col width="65px">
  	      <tr align="center">
  	      	<td colspan="5">Covariance \(C\)</td>
  	      </tr>
  	      <tr align="center">
    		    <td></td>
    		    <td>\(x_1\)</td>
    		    <td>\(x_2\)</td>
    		    <td>\(x_3\)</td>
    		    <td>\(x_4\)</td>
    		  </tr>
    		  <tr align="center">
    		    <td>\(x_1\)</td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor11" value="1"></td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor12" value="0"></td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor13" value="0"></td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor14" value="0"></td>
    		  </tr>
    		  <tr align="center">
    		    <td>\(x_2\)</td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor21" value="0"></td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor22" value="1"></td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor23" value="0"></td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor24" value="0"></td>
    		  </tr>
    		  <tr align="center">
    		    <td>\(x_3\)</td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor31" value="0"></td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor32" value="0"></td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor33" value="1"></td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor34" value="0"></td>
    		  </tr>
    		  <tr align="center">
    		    <td>\(x_4\)</td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor41" value="0"></td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor42" value="0"></td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor43" value="0"></td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_cor44" value="1"></td>
    		  </tr>
    		</table>

    		<table style="width:110px;display:inline;margin-right:25px;border:none;">
  	      <col width="50px">
  	      <col width="65px">
  	      <tr align="center">
  	      	<td colspan="2" style="border:none;">Foreground</td>
  	  	  </tr>
  	      <tr align="center">
  	      	<td colspan="2">Sample</td>
  	  	  </tr>
    		  <tr align="center">
    		    <td>\(x_1\)</td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_x1" value="1"></td>
    		  </tr>
    		  <tr align="center">
    		    <td>\(x_2\)</td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_x2" value="1"></td>
    		  </tr>
    		  <tr align="center">
    		    <td>\(x_3\)</td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_x3" value="1"></td>
    		  </tr>
    		  <tr align="center">
    		    <td>\(x_4\)</td>
    		    <td style="font-size:12px;"><input class="w3-input w3-border w3-round" type="text" id="ex2_x4" value="1"></td>
    		  </tr>
    		</table>
      </div>

      <div align=center style="margin-bottom:20px">
  	  	<table style="width:300px;display:inline;margin-right:25px;border:none;">
  	      <col width="180px">
  	      <col width="80px">
  	      <col width="80px">
  	      <col width="80px">
  	      <col width="80px">
  	      <tr align="center">
  	      	<td></td>
  	      	<td>\(\phi_1\)</td>
  	      	<td>\(\phi_2\)</td>
  	      	<td>\(\phi_3\)</td>
  	      	<td>\(\phi_4\)</td>
  	  	  </tr>
    		  <tr align="center">
    		    <td>SHAP Values (CE)</td>
    		    <td id="ex2_CE_phi1">1</td>
    		    <td id="ex2_CE_phi2">2</td>
    		    <td id="ex2_CE_phi3">3</td>
    		    <td id="ex2_CE_phi4">4</td>
    		  </tr>
    		  <tr align="center">
    		  	<td>SHAP Values (ICE)</td>
    		    <td id="ex2_ICE_phi1">1</td>
    		    <td id="ex2_ICE_phi2">2</td>
    		    <td id="ex2_ICE_phi3">3</td>
    		    <td id="ex2_ICE_phi4">4</td>
    		  </tr>
  		  </table>
      </div>

      <div align=center style="margin-bottom: 10px">
    		<input type="button" value="Preset A" class="w3-button w3-teal" id="ex2_presetA" onclick="ex2_presetA()">
      	<input type="button" value="Preset B" class="w3-button w3-green" id="ex2_presetB" onclick="ex2_presetB()">
      	<input type="button" value="Preset C" class="w3-button w3-green" id="ex2_presetC" onclick="ex2_presetC()">
      	<input type="button" value="Preset D" class="w3-button w3-green" id="ex2_presetD" onclick="ex2_presetD()">
      </div>

	    <p align="center" id="ex2_preset_text" style="font-size:13px">Independent variables.</p>

      <figcaption>
        <a href="#linear_shap_ex" class="figure-number">4</a>: Comparing two versions of SHAP values: conditional expectation (CE) and interventional conditional expectation (ICE).  We make two simplifying assumptions:

        <ul>
          <li class="no-space">The function is linear (\(f=\beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4\))</li>
          <li class="no-space">The data-generating distribution is multivariate normal (\(D\sim \mathcal{N}_4(0,C)\))</li>
        </ul>
      </figcaption>
    </figure>


  <p>
    In general, computing the conditional expectation SHAP value is difficult; however, for a multivariate normal distribution the conditional expectation is easy to calculate.  In addition, for a linear function the conditional expectation of the function equals the function applied to the conditional expectation (\(E[f(x)|x_S]=f(E[x|x_S])\)<d-footnote>Note that this is not generally true.  Only for affine functions.</d-footnote>).  
  </p>

  <!-- Proof -->
  <div">
    <p><a onclick="hideshow('proof_linearmodel')"><strong>+ Technical details</strong></a></p>

    <div id="proof_linearmodel", style="display:none">
      <p>
        Computing SHAP values for a linear model is much easier than for other model classes <d-footnote>Note that we assume features have zero mean in the following calculations.</d-footnote>.  This is how we compute them for <a href="#linear_shap_ex" class="figure-number">4</a>.
      </p>

      <p>
        First, to compute Interventional Conditional Expectation (ICE) SHAP values for a linear model, we can start with:
        $$
        \begin{aligned}
        \phi_i(f,x)&= \sum_{S\in C } W(|S|,|N|)(\mathbb{E}_{D}[f(x)|do(x_{S\cup \{i\}})] {-} \mathbb{E}_{D}[f(x)|do(x_{S})])\\
        &=\sum_{S\in C } W(|S|,|N|)(\mathbb{E}_{D}[\beta x|do(x_{S\cup \{i\}})] {-} \mathbb{E}_{D}[\beta x|do(x_{S})])\\
        &=\sum_{S\in C } W(|S|,|N|) \beta (\mathbb{E}_{D}[x|do(x_{S\cup \{i\}})] {-} \mathbb{E}_{D}[x|do(x_{S})])\\
        &=\sum_{S\in C } W(|S|,|N|) \beta_i x^f_i\\ 
        &= \beta_i x^f_i\\ 
        \end{aligned}
          $$
          Note that \(\sum_{S\in C } W(|S|,|N|)=1\).
    </p>

      <p>
        Second, to compute Conditional Expectation (CE) SHAP values for a linear model, we assume the background distribution \(D\) is multivariate normal with zero mean and covariance \(C\), we can start with:
        $$
        \begin{aligned}
        \phi_i(f,x)&= \sum_{S\in C } W(|S|,|N|)(\mathbb{E}_{D}[f(x)|x_{S\cup \{i\}}] {-} \mathbb{E}_{D}[f(x)|x_{S}])\\
        &= \sum_{S\in C } W(|S|,|N|)(\mathbb{E}_{D}[\beta x|x_{S\cup \{i\}}] {-} \mathbb{E}_{D}[\beta x|x_{S}])\\
        &= \sum_{S\in C } W(|S|,|N|)\beta (\mathbb{E}_{D}[x|x_{S\cup \{i\}}] {-} \mathbb{E}_{D}[x|x_{S}])\\
        \end{aligned}
        $$
        Here, we know that \(\mathbb{E}_{D}[x|x_{S}]=C_{N\setminus S,S} C_{S,S}^{-1} x_{S}\).
      </p>
      <hr style="margin-top:0px;margin-bottom:20px;">
    </div>

  <p>
    In <a href="#linear_shap_ex" class="figure-number">4</a> we highlight tradeoffs between the conditional expectation and the interventional conditional expectation for a linear model.  Comparing A to B, we can see that the correlation between variables will cause the CE SHAP values to be split between correlated variables.  Although this may be desirable at times<d-footnote>For example, if we are interested in detecting whether a model is secretly using a confounding variable, we may want CE SHAP values.</d-footnote>, it can feel somewhat unnatural as in D, where feature \(x_4\) is as important as feature \(x_3\) despite not being used in the model.
  </p>

    <p>
      Although both CE SHAP values and ICE SHAP values are meaningful, we will focus on ICE SHAP values moving forward because they are tractable to compute and explain what the model is doing, rather than how features in the data relate to each other.
    </p>

	</div>

    <!-- Background distribution -->
    <h3 id="background_distribution">1.5 Single reference SHAP values</h3>

	<p>
		To compute \(\phi_i(f,x^f)\) we evaluate the interventional conditional expectation.  However, this depends on a <i>background distribution</i> \(D\) that the foreground sample will be compared to.  
	</p>

	<p>
		One natural definition of the background distribution is a uniform distribution over a population sample.  For instance, in machine learning, you could assign equal probability to every sample in your training set.  With this background distribution, we can re-write the SHAP value as an average of <i>single reference SHAP values</i> (which are analogous to <a href="#masking_missing">masking missing features</a>) <d-cite bibtex-key="chen2019explaining"></d-cite>:
		$$
		\phi_i(f,x^f)=\frac{1}{|D|}\sum_{x^b\in D}\phi_i(f,x^f,x^b)
		$$
		This is in part because the interventional conditional expectation has a very natural definition when the background distribution is a single sample \(x^b\) (\(D_{x^b}\)):
		$$
		\mathbb{E}_{D_{x^b}}[f(x^f)|\text{do}(x_{S})]=h^S\text{, where } h^S_i = x^f_i\text{ if }i\in S\text{, }x^b_i\text{ otherwise}
		$$
		In words, the interventional conditional expectation for a single baseline is the model prediction for a hybrid sample where the features in \(S\) are from the explicand and the remaining features are from baseline.  This is as if <i>we intervene on features in the foreground sample with features from the background sample</i>.  This should look very familiar, as it is equivalent to obtaining the SHAP value by treating a sample in the background distribution as a baseline and masking (<a href="#masking_missing">Section 1.3</a>).
	</p>  

	<!-- Background Distribution Proof -->
	<div">
		<p><a onclick="hideshow('proof_backgrounddist')"><strong>+ Technical details</strong></a></p>

		<div id="proof_backgrounddist", style="display:none">

		    <p>
			    Define \(C\) to be all combinations of the set \(N \setminus \{i\}\) and \(P\) to be all permutations of \(N \setminus \{i\}\).  Starting with the definition of SHAP values: 
				$$
				\begin{aligned}
				\phi_i(f,x^f)&= \sum_{S\in C } W(|S|,|N|)(\mathbb{E}_{D}[f(x^f)|\text{do}(x_{S\cup \{i\}})] {-} \mathbb{E}_{D}[f(x^f)|\text{do}(x_{S})])\\
				&=\frac{1}{|P|}\sum_{S\subseteq P} \mathbb{E}_D[f(x^f)|\text{do}(x_{S \cup \{i\}})] {-} \mathbb{E}_D[f(x^f)|\text{do}(x_{S})]\\
				&= \frac{1}{|P|}\sum_{S\subseteq P}\frac{1}{|D|}\sum_{x^b\in D} f(h^{S\cup \{i\}}) {-} f(h^{S})\\
				&= \frac{1}{|D|}\sum_{x^b\in D} \frac{1}{|P|}\sum_{S\subseteq P} f(h^{S\cup \{i\}}) {-} f(h^{S})\\
				&= \frac{1}{|D|}\sum_{x^b\in D} \underbrace{\sum_{S\subseteq C} W(|S|,|N|)f(h^{S\cup \{i\}}) {-} f(h^{S})}_{\text{Single reference SHAP value}}\\
				&=\frac{1}{|D|}\sum_{x^b\in D}\phi_i(f,x^f,x^b)
				\end{aligned}
				$$
			</p>
		</div>
    <hr style="margin-top:0px;margin-bottom:20px;">
	</div>

	<p>
		In summary, we reduce the problem of obtaining \(\phi_i(f,x^f)\) to an average of simpler problems \(\phi_i(f,x^f,x^b)\) where our foreground sample \(x^f\) is compared to a distribution with only one background sample \(x^b\).  This new problem formulation will prove to be an easy problem to tackle for tree models.
	</p>

  </div>

  <hr>


  <!-- The Algorithm Section -->
  <div style="max-width:800px">
    <h2 id="algorithm">2. An algorithm to explain trees quickly and exactly</h2>
    
    <p>
    Now our goal is to tackle the simpler problem of obtaining single reference SHAP values \(\phi_i(f,x^f,x^b)\) that are attributions for a single foreground sample and background sample.  The examples in this section are all for a specific foreground sample, background sample, and tree specified in <a href="#define_tree_samples" class="figure-number">5</a>.
	</p>

	<figure id="define_tree_samples">
		<p align=center style="font-size:15px;">
			Tree Parameters <d-footnote>Green links indicate the foreground sample goes down a particular split, red indicates the background sample does, and blue indicates both samples do.</d-footnote> (Click nodes)</strong> <br>
		</p>

    <div style="max-width:800px;height:320px;text-align:center">
      <div style="width:350px;height:300px;display:inline-block;">

        <!-- The SVG tree itself -->
        <div align=center>
      	  <div id="graphic"></div> 
      	  <div class="tooltip" id="tooltip"></div>
        </div>

      </div>

      <div style="width:180px;height:300px;display:inline-block;vertical-align: top;">

        <!-- Select variable/threshold -->
        <div align=center style="margin-bottom: 20px">
          <p id="ex3_var_label" align=center style="font-size: 15px">Node Variable:</p>
          <div align=center>
            <input type="button" value="x1" class="w3-button w3-teal" id="ex3_x1_select" onclick="ex3_select_feat1()" style="font-size: 15px">
            <input type="button" value="x2" class="w3-button w3-green" id="ex3_x2_select" onclick="ex3_select_feat2()" style="font-size: 15px">
            <input type="button" value="x3" class="w3-button w3-green" id="ex3_x3_select" onclick="ex3_select_feat3()" style="font-size: 15px">
          </div>
        </div>

        <div id="ex3_thres_div" align=center style="margin-bottom: 20px">
          <p style="font-size: 15px">Node Threshold:</p>
          <input type="range" min="-15" max="15" value="5" class="slider" id="ex3_thres" style="width: 130px;">
          <p style="font-size: 15px;width: 20px;" id="ex3_thres_out">5</p>
        </div>

        <div id="ex3_val_div" align=center style="margin-bottom: 20px;display: none;">
          <p style="font-size: 15px">Leaf Value:</p>
          <input type="range" min="-15" max="15" value="5" class="slider" id="ex3_val" style="width: 130px;">
          <p style="font-size: 15px;width: 20px;" id="ex3_val_out">5</p>
        </div>
      </div>
    
    </div>

    <!-- Foreground and background samples -->
    <div align=center style="margin-bottom: 20px;">
      <table style="width:500px;display:inline;border:none;">
        <col width="100px"/>
        <col width="150px"/>
        <col width="50px"/>
        <col width="150px"/>
        <col width="50px"/>
        <tr>
          <td>Variable</td>
          <td colspan="2">Foreground Sample \(x^f\)</td>
          <td colspan="2">Background Sample \(x^b\)</td>
        </tr>
        <tr>
          <td>\(x_1\)</td>
          <td><input type="range" min="-15" max="15" value="0" class="slider" id="fx1"></td>
          <td align="left" id="fx1_out"></td>
          <td><input type="range" min="-15" max="15" value="10" class="slider" id="bx1"></td>
          <td align="left" id="bx1_out"></td>
        </tr>
        <tr>
          <td>\(x_2\)</td>
          <td><input type="range" min="-15" max="15" value="0" class="slider" id="fx2"></td>
          <td align="left" id="fx2_out"></td>
          <td><input type="range" min="-15" max="15" value="10" class="slider" id="bx2"></td>
          <td align="left" id="bx2_out"></td>
        </tr>
        <tr>
          <td>\(x_3\)</td>
          <td><input type="range" min="-15" max="15" value="10" class="slider" id="fx3"></td>
          <td align="left" id="fx3_out"></td>
          <td><input type="range" min="-15" max="15" value="0" class="slider" id="bx3"></td>
          <td align="left" id="bx3_out"></td>
        </tr>
      </table>
    </div>


    <div align=center style="margin-bottom: 10px">
      <input type="button" value="Reset" class="w3-button w3-green" onclick="ex3_reset()">
    </div>

    <figcaption>
      <a href="#define_tree_samples" class="figure-number">5</a>: Choose foreground sample \(x^f\), background sample \(x^b\), and tree parameters for the remainder of <a href="#algorithm">Section 2</a>.  This is the tree, foreground sample, and background sample that will be explained in the following examples.
    </figcaption>

	</figure>

    <!-- Brute force -->
    <h3 id="brute_force">2.1 Brute force</h3>

    <p>
    	Based on the proof in <a href="#background_distribution">Section 1.5</a>, the brute force approach would be to compute the following:

    	$$
    	\phi_i(f,x^f,x^b)=\sum_{S\subseteq N\setminus\{i\}} \underbrace{W(|S|,|N|)}_{W}\underbrace{f(h^{S\cup \{i\}})}_{\text{\textcolor{green}{Pos} term}} {-} \underbrace{f(h^S)}_{\text{\textcolor{red}{Neg} term}})
    	$$

    	The algorithm is then fairly straightforward:
    </p>

  <figure id="brute_force_ex" style="margin-bottom: 0px">
    <div class="container" style="width:700px">

      <div class="first" style="width:350px;height:410px">
      	<p align=center style="font-size:15px;margin-bottom:10px">Tree Parameters</p>
      	<div align=center id="ex4_divtree" style="margin-top:0px;margin-bottom:0px"></div>

	    <table align=center style="border:none;">
	      <col width="20px"/>
	      <col width="80px"/>
	      <col width="20px"/>
	      <col width="80px"/>
	      <col width="20px"/>
	      <col width="80px"/>
	      <tr>
	        <td align=center colspan="6">Attribution Values</td>
	      </tr>
	      <tr>
	        <td id="ex4_phi1">\(\phi_1\)</td>
	        <td align=center id="ex4_phi1_val"></td>
	        <td id="ex4_phi2">\(\phi_2\)</td>
	        <td align=center id="ex4_phi2_val"></td>
	        <td id="ex4_phi3">\(\phi_3\)</td>
	        <td align=center id="ex4_phi3_val"></td>
	      </tr>
	    </table>
      	</div>
      </div>

      <div class="second" style="width:350px;height:410px">
        <table align=center style="border:none;margin-bottom:-5px">
          <col width="60px"/>
          <col width="90px"/>
          <col width="90px"/>
          <col width="90px"/>
          <tr>
            <td align=center colspan="4">Foreground & background sample</td>
          </tr>
          <tr>
            <th></th>
            <th>\(x_1\)</th>
            <th>\(x_2\)</th>
            <th>\(x_3\)</th>
          </tr>
          <tr>
            <td>\(x^f\)</td>
            <td id="ex4_fx1">0</td>
            <td id="ex4_fx2">0</td>
            <td id="ex4_fx3">10</td>
          </tr>
          <tr>
            <td>\(x^b\)</td>
            <td id="ex4_bx1">10</td>
            <td id="ex4_bx2">10</td>
            <td id="ex4_bx3">0</td>
          </tr>
          <tr id="ex4_hS">
            <td>\(h^S\)</td>
            <td id="ex4_hs1"></td>
            <td id="ex4_hs2"></td>
            <td id="ex4_hs3"></td>
          </tr>
          <tr id="ex4_hSi">
            <td>\(h^{S\cup i}\)</td>
            <td id="ex4_hsi1"></td>
            <td id="ex4_hsi2"></td>
            <td id="ex4_hsi3"></td>
          </tr>
        </table>

        <table align=center style="border:none;margin-bottom:-5px">
          <col width="50px"/>
          <col width="67px"/>
          <col width="67px"/>
          <col width="67px"/>
          <col width="67px"/>
          <tr>
            <td align=center colspan="5">Algorithm State</td>
          </tr>
          <tr>
            <td>\(W\)</td>
            <td>\(S\)</td>
            <td id="ex4_fxS">\(f(h^S)\)</td>
            <td>\(S\cup{i}\)</td>
            <td id="ex4_fxSi">\(f(h^{S\cup{i}})\)</td>
          </tr>
          <tr id="ex4_s1_row">
            <td>1/3</td>
            <td id="s1"></td>
            <td id="s1_val"></td>
            <td id="s1i"></td>
            <td id="s1i_val"></td>
          </tr>
          <tr id="ex4_s2_row">
            <td>1/6</td>
            <td id="s2"></td>
            <td id="s2_val"></td>
            <td id="s2i"></td>
            <td id="s2i_val"></td>
          </tr>
          <tr id="ex4_s3_row">
            <td>1/6</td>
            <td id="s3"></td>
            <td id="s3_val"></td>
            <td id="s3i"></td>
            <td id="s3i_val"></td>
          </tr>
          <tr id="ex4_s4_row">
            <td>1/3</td>
            <td id="s4"></td>
            <td id="s4_val"></td>
            <td id="s4i"></td>
            <td id="s4i_val"></td>
          </tr>
        </table>
	  </div>

      <div class="clear"></div>
    </div>

    <div align=center style="margin-bottom: 20px">
      <dt-code id="bf_1" block="" language="python">
        def ite_brute(array xf, array xb, tree T, array N):
      </dt-code>
      <dt-code id="bf_2" block="" language="python">
          phi = [0]*len(xf)
      </dt-code>
      <dt-code id="bf_3" block="" language="python">
          for each feature i in N:
      </dt-code>
      <dt-code id="bf_4" block="" language="python">
            for each set S in powerset(setminus(N,i)):
      </dt-code>
      <dt-code id="bf_5" block="" language="python">
              hs  = [xf[j] for j in N if j in S else xb[j]] # Hybrid samples
      </dt-code>
      <dt-code id="bf_6" block="" language="python">
              hsi = [xf[j] for j in N if j in union(S,i) else xb[j]]
      </dt-code>
      <dt-code id="bf_7" block="" language="python">
              fxs  = T.predict(hs) # Predictions
      </dt-code>
      <dt-code id="bf_8" block="" language="python">
              fxsi = T.predict(hsi)
      </dt-code>
      <dt-code id="bf_9" block="" language="python">
              W = (len(S)!*(len(N)-len(S)-1)!)/(len(N)!) # Weight
      </dt-code>
      <dt-code id="bf_10" block="" language="python">
              phi[i] += W*(fxsi-fxs) # Calculate phi contribution
      </dt-code>
    </div>

    <div align=center style="margin-bottom: 0px">
      <input type="button" class="w3-button w3-green" value="<<" onclick="ex4_reset()">
      <input type="button" class="w3-button w3-green" value=">" onclick="bruteForceStep()">
      <input type="button" class="w3-button w3-green" value=">>" onclick="bruteForceRunAll()">
    </div>
  </figure>

  <figure>
    <figcaption>
      <a href="#brute_force_ex" class="figure-number">6</a>: Brute force algorithm for the tree and samples specified in <a href="#define_tree_samples" class="figure-number">5</a>.
    </figcaption>
  </figure>

    <p>
    	If we assume the computational cost of computing the weight \(W\) is constant<d-footnote>Which it will be if we memoize \(k!\) for \(k=1,\cdots,d\).</d-footnote>, then the complexity of the brute force method is the number of terms in the summation multiplied by the cost of making a prediction (on the order of the depth of the tree).  This gives \(O((\text{tree depth})\times 2^{d})\).
    </p>

    <p>
    	Then, in order to compute \(\phi_i(f,x^f,x^b)\) for all features \(i\), we have to re-run the entire algorithm \(d\) times, giving us an overall complexity of
    	$$
    	O(d\times (\text{tree depth})\times 2^{d})
    	$$
	</p>


	<p>
	    An exponential computational complexity is bad; however, if we <i>constrain \(f(x)\) to be a tree-based model</i> (e.g., XGBoost, decision trees, random forests, etc.), then we can come up with a polynomial time algorithm to compute \(\phi_i(f,x^f,x^b)\) exactly.  Why is this the case?  Well, looking at <a href="#brute_force_ex" class="figure-number">6</a>, we can see that even for explaining a single feature, the brute force algorithm may consider a particular path multiple times.  However, to compute the SHAP value for a single feature, it turns out that we only need to consider each path once.  This insight leads us to the naive algorithm in <a href="#naive_implementation">Section 2.2</a>.
	</p>


    <!--                      -->
    <!-- Naive Implementation -->
    <!--                      -->

    <h3 id="naive_implementation">2.2 Naive Implementation</h3>

	<p>
		Before we get into the algorithm, we first describe a theorem that is the basis for this naive implementation.
	</p>    

	<p style="margin-bottom: 0px">
  	<strong>Theorem 1: To calculate \(\phi_i(f,x^f,x^b)\), we can calculate attributions for each path from the root to each leaf.</strong>  
  </p>

  <p>
    For a given path \(P\), we define \(N_P\) to be the unique features in the path and \(S_P\) to be the unique features in the path that came from \(x^f\).  Finally, define \(v\) to be the value of the path's leaf.  Then, the attribution of the path is:

  	$$
		\phi_i^P(f,x^f,x^b)=
    \begin{cases}
    	0 & \text{if}\ i\notin N_P \\
    	\textcolor{green}{W(|S_P|-1,|N_P|)\times v} & \text{if}\ i\in S_P \\
    	\textcolor{red}{-W(|S_P|,|N_P|)\times v} & \text{o.w.}
    \end{cases}
  	$$
	</p>

	<!-- Proof -->
	<p><a onclick="hideshow('naive_alg_proof')"><strong>+ Technical Details</strong></a></p>
	<div id="naive_alg_proof", style="display:none">
		<p>
			<i>Sketch of proof</i>: Treat each path \(P\) in the tree from the root to each leaf as a separate model \(f^P(x)\) that returns the value of the leaf if that path is traversed by \(x\) or zero otherwise.  This results in (\(\text{\# leaf nodes}\)) models that operate on disjoint parts of the input space, implying that our original model is equal to the summation of all of these path models:
      $$f(x)=\sum_P f^P(x)$$ 
      By the additivity of SHAP values, 
      $$\phi_i(f,x^f,x^b)=\sum_P\phi_i(f^P,x^f,x^b)$$  
      Then, we can simply calculate \(\phi_i\) for each path model.  Since the path model is zero everywhere except for the associated path, we arrive to the solution in Theorem 1.
		</p>
    <hr style="margin-top:0px;margin-bottom:20px;">
	</div>

	<p>
		Then the goal of the algorithm is to obtain \(N_P\) and \(S_P\) for each path by recursively traversing the tree.  We will start by explaining the algorithm via an example:
	</p>

	<figure id="tree_example_cases" align=center>
		<img src="images/tree_example3.png" alt="Tree Example 3" style="width: 360px;margin-bottom: 20px;">
    <figcaption><a href="#tree_example_cases" class="figure-number">7</a>:  Green paths are associated with \(\textcolor{green}{x^f}\), red paths are \(\textcolor{red}{x^b}\), and blue paths are associated with both.</figcaption>
	</figure>

	<p>
		In the naive algorithm, we maintain lists \(N_P\) and \(S_P\) as we traverse the tree.  At each internal node (Cases 2-4) we update the lists and then pass them to the node's children.  At the leaf nodes (Case 1), we calculate the attribution for each path.  In Figure 3, we see four possible cases:
		<ul>
			<li class="no-space">Case 1: \(n\) is a leaf</li>
			<ul class="no-space">
				<li class="no-space">Return the attribution in Theorem 1 based on \(N_P\) and \(S_P\)</li>
			</ul>

			<li class="no-space">Case 2: The feature has been encountered already (\(n_{feature}\in N_P\))</li>
			<ul class="no-space">
				<li class="no-space">Depending on if we split on \(x^f\) or \(x^b\), we compare either \(x^f_{n_{feature}}\) or \(x^b_{n_{feature}}\) to \(n_{threshold}\) and go down the appropriate child</li>
				<li class="no-space">Pass down \(N_P\) and \(S_P\) without modifications because we did not add a new feature</li>
			</ul>

			<li class="no-space">Case 3: Both \(x^f\) and \(x^b\) are on the same side of \(n\)'s split</li>
			<ul class="no-space">
				<li class="no-space">Pass down \(N_P\) and \(S_P\) without modifications because relative to \(x^f\) and \(x^b\) it's as if this node doesn't exist</li>
			</ul>

			<li class="no-space">Case 4: \(x^f\) and \(x^b\) go to different children</li>
			<ul class="no-space">
				<li class="no-space">Add \(n_{feature}\) to both \(N_P\) and \(S_P\) and pass both lists to the \(x^f\) child</li>
				<li class="no-space">Only add \(n_{feature}\) to \(N_P\) and pass both lists to the \(x^b\) child</li>
			</ul>
		</ul>
	</p>

  <figure id="naive_ex" class="l-middle" style="margin-bottom: -10px;">

  	<div class="container" style="width:1000px">
    	<div class="first" style="width:400px;height:620px">
        <p align=center style="font-size:15px;margin-bottom:10px">Tree Parameters</p>
        <div id="ex5_divtree" style="margin-top:0px;margin-bottom:-5px;"></div>
            
  			<table align=center style="border:none;">
          <col width="60px"/>
          <col width="90px"/>
          <col width="90px"/>
          <col width="90px"/>
          <tr>
            <td align=center colspan="4">Foreground & background sample</td>
          </tr>
    			<tr>
    				<td></td>
    				<td>\(x_1\)</td>
    				<td>\(x_2\)</td>
    				<td>\(x_3\)</td>
    			</tr>
    			<tr>
    				<td>\(x^f\)</td>
    				<td id="ex5_fx1">0</td>
    				<td id="ex5_fx2">0</td>
    				<td id="ex5_fx3">10</td>
    			</tr>
    			<tr>
    				<td>\(x^b\)</td>
    				<td id="ex5_bx1">10</td>
    				<td id="ex5_bx2">10</td>
    				<td id="ex5_bx3">0</td>
    			</tr>
    			<tr id="ex5_h">
    				<td>\(h\)</td>
    				<td id="ex5_h1"></td>
    				<td id="ex5_h2"></td>
    				<td id="ex5_h3"></td>
    			</tr>
  			</table>

        <table align=center style="border:none;">
          <col width="40px"/>
          <col width="120px"/>
          <col width="40px"/>
          <col width="120px"/>
          <td align=center colspan="4">Algorithm State</td>
          <tr>
            <td>\(S_P\)</td>
            <td id="ex5_sp"></td>
            <td>\(N_P\)</td>
            <td id="ex5_np"></td>
          </tr>
        </table>

        <table align=center style="border:none;">
          <col width="20px"/>
          <col width="80px"/>
          <col width="20px"/>
          <col width="80px"/>
          <col width="20px"/>
          <col width="80px"/>
          <tr>
            <td align=center colspan="6">Attribution Values</td>
          </tr>
          <tr>
            <td id="ex5_phi1">\(\phi_1\)</td>
            <td align=center id="ex5_phi1_val"></td>
            <td id="ex5_phi2">\(\phi_2\)</td>
            <td align=center id="ex5_phi2_val"></td>
            <td id="ex5_phi3">\(\phi_3\)</td>
            <td align=center id="ex5_phi3_val"></td>
          </tr>
        </table>
      </div>

	  <div class="second" style="width:600px;height:620px;margin-top:20px;">
        <dt-code id="naivecode_1" class="indent0" block="" language="python">
        def ite_naive(array xf, array xb, tree T):
        </dt-code>
        <dt-code id="naivecode_2" class="indent1" block="" language="python">
          phi = [0]*len(xf)
        </dt-code>
        <dt-code id="naivecode_3" class="indent1" block="" language="python">
          def recurse(node n, list np, list sp):
        </dt-code>
        <dt-code id="naivecode_4" class="indent2" block="" language="python">
            # Case 1: Leaf
        </dt-code>
        <dt-code id="naivecode_5" class="indent2" block="" language="python">
            if n.is_leaf: [Theorem 1]
        </dt-code>
        <dt-code id="naivecode_6" class="indent3" block="" language="python">
              for i in N:
        </dt-code>
        <dt-code id="naivecode_7" class="indent4" block="" language="python">
                if i in sp:
        </dt-code>
        <dt-code id="naivecode_8" class="indent5" block="" language="python">
                  phi[i] += W(len(sp)-1,len(np)) 
        </dt-code>
        <dt-code id="naivecode_9" class="indent4" block="" language="python">
                elif:
        </dt-code>
        <dt-code id="naivecode_10" class="indent5" block="" language="python">
                  phi[i] -= W(len(sp),len(np)) 
        </dt-code>
        <dt-code id="naivecode_11" class="indent2" block="" language="python">
            # Find children associated with xf and xb
        </dt-code>
        <dt-code id="naivecode_12" class="indent2" block="" language="python">
            xf_child = n.left if xf[n.feat] < n.thres else n.right
        </dt-code>
        <dt-code id="naivecode_13" class="indent2" block="" language="python">
            xb_child = n.left if xb[n.feat] < n.thres else n.right

        </dt-code>
        <dt-code id="naivecode_14" class="indent2" block="" language="python">
            # Case 2: Feature encountered before
        </dt-code>
        <dt-code id="naivecode_15" class="indent2" block="" language="python">
            if n.feat in np:
        </dt-code>
        <dt-code id="naivecode_16" class="indent3" block="" language="python">
              if n.feat in sp:
        </dt-code>
        <dt-code id="naivecode_17" class="indent4" block="" language="python">
                return(recurse(xf_child,np,sp))
        </dt-code>
        <dt-code id="naivecode_18" class="indent3" block="" language="python">
              else:
        </dt-code>
        <dt-code id="naivecode_19" class="indent4" block="" language="python">
                return(recurse(xb_child,np,sp))

        </dt-code>
        <dt-code id="naivecode_20" class="indent2" block="" language="python">
            # Case 3: xf and xb go the same way
        </dt-code>
        <dt-code id="naivecode_21" class="indent2" block="" language="python">
            if xf_child == xb_child:
        </dt-code>
        <dt-code id="naivecode_22" class="indent3" block="" language="python">
              return(recurse(xf_child,np,sp))

        </dt-code>
        <dt-code id="naivecode_23" class="indent2" block="" language="python">
            # Case 4: xf and xb don't go the same way
        </dt-code>
        <dt-code id="naivecode_24" class="indent2" block="" language="python">
            if not xf_child != xb_child:
        </dt-code>
        <dt-code id="naivecode_25" class="indent3" block="" language="python">
              f_phi = recurse(xf_child,np+[n.feat],sp+[n.feat])
        </dt-code>
        <dt-code id="naivecode_26" class="indent3" block="" language="python">
              b_phi = recurse(xb_child,np+[n.feat],sp)
        </dt-code>
        <dt-code id="naivecode_27" class="indent1" block="" language="python">
          recurse(n=T.root,sp=[],np=[])
        </dt-code>
      </div>

      <div class="clear"></div>
		</div>

    <div align=center>
    	<input type="button" class="w3-button w3-green" value="<<" onclick="ex5_reset()">
    	<input type="button" class="w3-button w3-green" value=">" onclick="naiveStep()">
    	<input type="button" class="w3-button w3-green" value=">>" onclick="naiveRunAll()">
    </div>

    <!-- <p align=center id="ex5_naive_step">Naive algorithm</p> -->
  </figure>

  <figure>

    <figcaption>
      <a href="#naive_ex" class="figure-number">8</a>: Naive algorithm for the tree and samples specified in <a href="#define_tree_samples" class="figure-number">5</a>.
    </figcaption>

	</figure>

	<!-- Pseudocode -->
<!-- 	<div style="padding-left:10px;padding-right:10px">

		<p><a onclick="hideshow('pseudocode_naive')"><strong>+ Pseudocode</strong></a></p>

		<div id="pseudocode_naive" style="display:none">
      <dt-code block language="python">
        def ite_naive(array xf, array xb, tree T):
          def recurse(node n, list np, list sp):
            // Case 1: Leaf
            if n.is_leaf:
              return(phi_i_p) [based on Theorem 1]

            // Find children associated with xf and xb
            xf_child = n.left if xf[n.feat] < n.thres else n.right
            xb_child = n.left if xb[n.feat] < n.thres else n.right

            // Case 2: Feature encountered before
            if n.feat in np:
              if n.feat in sp:
                return recurse(xf_child,np,sp)
              else:
                return recurse(xb_child,np,sp)

            // Case 3: xf and xb go the same way
            if xf_child == xb_child:
              return(recurse(xf_child,np,sp))

            // Case 4: xf and xb don't go the same way
            if not xf_child != xb_child:
              f_phi = recurse(xf_child,np+[n.feat],sp+[n.feat])
              b_phi = recurse(xb_child,np+[n.feat],sp)
              return(b_phi+f_phi)

          return(recurse(n=T.root,sp=[],np=[]))
      </dt-code>
    </div>
	</div>
 -->
    <p>
		Then, we examine the computational complexity to compute \(\phi_i(f,x^f,x^b)\) for all features \(i\) using the naive algorithm.
	</p>

	<p>
		First of all, the worst case complexity for each internal node is based on Cases 2-4.  In the worst case, we need to check whether the current node's feature has been encountered previously by iterating through \(S_P\) and \(N_P\).  Since these lists are of length \((\text{tree depth})\) in the worst case, each node incurs a linear \(O(\text{tree depth})\) cost.
	</p>

	<p>
		Next, for the leaf nodes, we compute the contributions for each feature (of which there are \(d\)).  Then, computing the contributions for each feature requires checking whether the feature is in \(S_P\).  This means that each leaf node incurs a cost of \(O((\text{tree depth})\times d)\) cost.
	</p>

	<p>
		Putting this together, we get an overall cost of:
		$$
		O((\text{\# internal nodes})\times (\text{tree depth})) + O((\text{\# leaf nodes})\times (\text{tree depth}) \times d)
		$$
	</p>

	<p>
		In the next section we present two observations that allows us to compute \(\phi_i(f,x^f,x^b)\) for all features \(i\) in \(O(\text{\# nodes})\) time.
	</p>

    <!-- Dynamic Programming -->
    <h3 id="dynamic_programming_implementation">2.3 Dynamic Programming Implementation</h3>

    <p>
    	To improve the computational complexity of the straightforward naive algorithm, we can make two observations.
    </p>

    <p>
    	The first observation is that we can actually get rid of the multiplicative \((\text{tree depth})\) factor by focusing on what the lists \(S_P\) and \(N_P\) are used for.  The first use is to check if a feature has been previously encountered.  By replacing the lists with boolean arrays, we can check if a given feature has been encountered by a constant time access into the arrays.  This means the internal nodes now incur a constant cost.  The second use of \(S_P\) and \(N_P\) is to calculate the sizes of \(|S|\) and \(|N|\) at the leaves.  By instead maintaining integers that keep track of these values, the leaves no longer have to iterate through lists.  This gets rid of the multiplicative \((\text{tree depth})\) factor.  Then, the new computational complexity is:
		$$
		O((\text{\# internal nodes}) + O((\text{\# leaf nodes}) \times d)
		$$
    </p>

    <p>
	    The second observation is that we can compute the attributions for all features simultaneously as we traverse the tree by passing \(\textcolor{green}{\text{Pos}}\) and \(\textcolor{red}{\text{Neg}}\) attributions to parent nodes.  Before describing the algorithm in more detail, we first present a figure that illustrates why passing up the attributions is sufficient.
	</p>

	<figure id="collapsibility_fig">
		<figure align=center style="margin-bottom: 10px">
			<img align=center src="images/tree_example4.png" alt="Tree Example 3" style="width: 220px;">
		</figure>

		<table cellpadding="10" align="center" style="border:none;">
			<tr>
				<td align="left">\(\phi_1(f,x^f,x^b)\)</td>
				<td align="left">\(\textcolor{green}{\text{Pos}_1}+\textcolor{green}{\text{Pos}_2}+\textcolor{red}{\text{Neg}_3}+\textcolor{red}{\text{Neg}_4}\)</td>
			</tr>
			<tr>
				<td align="left">\(\phi_2(f,x^f,x^b)\)</td>
				<td align="left">\(\textcolor{red}{\text{Neg}_1}+\textcolor{green}{\text{Pos}_2}+\textcolor{green}{\text{Pos}_3}+\textcolor{red}{\text{Neg}_4}\)</td>
			</tr>
		</table>

    <figcaption><a href="#collapsibility_fig" class="figure-number">10</a> Example to illustrate collapsibility for features.  Green paths are associated with \(\textcolor{green}{x^f}\) and red paths are \(\textcolor{red}{x^b}\)</figcaption>
	</figure>



	<p>
		In Figure 4, we can first observe that for each leaf, according to Theorem 1, there are only two possible values needed to compute the SHAP values (\(\textcolor{green}{\text{Pos}}\) and \(\textcolor{red}{\text{Neg}}\)).  Based on the attributions for \(x_1\) we see that these \(\textcolor{green}{\text{Pos}}\) and \(\textcolor{red}{\text{Neg}}\) terms can be grouped by the left and right subtrees below \(x_1\).  To generalize this example, we make the following observation:
	</p>

	<p>
		<strong>Observation:  In order to compute the attribution for any feature \(i\) it is sufficient to consider the paths that correspond to each Case 4 node's children.</strong>  First, focusing on a specific Case 4 node \(n\), we know that one child is associated with \(x^f\) child and one child is associated with \(x^b\).  Then, the attribution to the node's feature is:
		$$
		\sum_{\text{paths }P\text{ under }x^f\text{ child}}\textcolor{green}{\text{Pos}_P} + \sum_{\text{paths }P\text{ under }x^b\text{ child}}\textcolor{red}{\text{Neg}_P}
		$$
	</p>

	<p>
		This observation suggests that we can always add the \(\textcolor{green}{\text{Pos}}\) and \(\textcolor{red}{\text{Neg}}\) terms at a given node and pass them up to the parent.  This information is sufficient to calculate the attributions for each upstream feature.  This aggregation of the \(\textcolor{green}{\text{Pos}}\) and \(\textcolor{red}{\text{Neg}}\) terms is the dynamic programming observation that allows each upstream node to only need a constant number of operations to compute its feature's attribution.
	</p>

	<p>
		Using this observation, we devise an algorithm that computes the attributions for all features simultaneously (getting rid of that final multiplicative \(d\) factor for the leaf nodes):
	</p>

    <figure id="dynamic_ex" class="l-middle" style="margin-bottom: -10px;">
      <div class="container" style="width:1000px">
        <div class="first" style="width:400px;height:620px">
        	<p align=center style="font-size:15px;margin-bottom:10px">Tree Parameters</p>
        	<div id="ex6_divtree" style="margin-top:0px;margin-bottom:-5px;"></div>

          <table align=center style="border:none;">
            <col width="60px"/>
            <col width="90px"/>
            <col width="90px"/>
            <col width="90px"/>
            <tr>
              <td align=center colspan="4">Foreground & background sample</td>
            </tr>
      			<tr>
      				<th></th>
      				<th>\(x_1\)</th>
      				<th>\(x_2\)</th>
      				<th>\(x_3\)</th>
      			</tr>
      			<tr>
      				<td>\(x^f\)</td>
      				<td id="ex6_fx1">0</td>
      				<td id="ex6_fx2">0</td>
      				<td id="ex6_fx3">10</td>
      			</tr>
      			<tr>
      				<td>\(x^b\)</td>
      				<td id="ex6_bx1">10</td>
      				<td id="ex6_bx2">10</td>
      				<td id="ex6_bx3">0</td>
      			</tr>
      			<tr id="ex6_h">
      				<td>\(h\)</td>
      				<td id="ex6_h1"></td>
      				<td id="ex6_h2"></td>
      				<td id="ex6_h3"></td>
      			</tr>
    			</table>

          <table align=center style="border:none;">
            <col width="40px"/>
            <col width="120px"/>
            <col width="40px"/>
            <col width="120px"/>
            <td align=center colspan="4">Algorithm State</td>
            <tr>
              <td>\(S_C\)</td>
              <td id="ex6_sc"></td>
              <td>\(N_C\)</td>
              <td id="ex6_nc"></td>
            </tr>
          </table>

          <table align=center style="border:none;">
            <col width="20px"/>
            <col width="80px"/>
            <col width="20px"/>
            <col width="80px"/>
            <col width="20px"/>
            <col width="80px"/>
            <tr>
              <td align=center colspan="6">Attribution Values</td>
            </tr>
            <tr>
              <td id="ex6_phi1">\(\phi_1\)</td>
              <td align=center id="ex6_phi1_val"></td>
              <td id="ex6_phi2">\(\phi_2\)</td>
              <td align=center id="ex6_phi2_val"></td>
              <td id="ex6_phi3">\(\phi_3\)</td>
              <td align=center id="ex6_phi3_val"></td>
            </tr>
          </table>
        </div>

        <div class="second" style="width:600px;height:620px;margin-top:20px">
            <dt-code id="dynamiccode_1" class="indent0" block="" language="python">
            def ite_dynamic(array xf, array xb, tree T):
            </dt-code>
            <dt-code id="dynamiccode_2" class="indent1" block="" language="python">
              phi = [0]*len(xf)
            </dt-code>
            <dt-code id="dynamiccode_3" class="indent1" block="" language="python">
              def recurse(node n, int nc, int sc, array fseen, array bseen):
            </dt-code>
            <dt-code id="dynamiccode_4" class="indent2" block="" language="python">
                # Case 1: Leaf
            </dt-code>
            <dt-code id="dynamiccode_5" class="indent2" block="" language="python">
                if n.is_leaf:
            </dt-code>
            <dt-code id="dynamiccode_6" class="indent3" block="" language="python">
                  if sc == 0: return((0,0))
            </dt-code>
            <dt-code id="dynamiccode_7" class="indent3" block="" language="python">
                  else: return((n.value*W(sc,nc-1),-n.value*W(sc,nc)))
            </dt-code>

            <dt-code id="dynamiccode_8" class="indent2" block="" language="python">
                # Find children associated with xf and xb
            </dt-code>
            <dt-code id="dynamiccode_9" class="indent2" block="" language="python">
                xf_child = n.left if xf[n.feat] < n.thres else n.right
            </dt-code>
            <dt-code id="dynamiccode_10" class="indent2" block="" language="python">
                xb_child = n.left if xb[n.feat] < n.thres else n.right
            </dt-code>
            <dt-code id="dynamiccode_11" class="indent2" block="" language="python">
                # Case 2: Feature encountered before
            </dt-code>
            <dt-code id="dynamiccode_12" class="indent2" block="" language="python">
                if fseen[n.feat] > 0:
            </dt-code>
            <dt-code id="dynamiccode_13" class="indent3" block="" language="python">
                  return(recurse(xf_child,nc,sc,fseen,bseen))
            </dt-code>
            <dt-code id="dynamiccode_14" class="indent2" block="" language="python">
                if bseen[n.feat] > 0:
            </dt-code>
            <dt-code id="dynamiccode_15" class="indent3" block="" language="python">
                  return(recurse(xb_child,nc,sc,fseen,bseen))
            </dt-code>

            <dt-code id="dynamiccode_16" class="indent2" block="" language="python">
                # Case 3: xf and xb go the same way
            </dt-code>
            <dt-code id="dynamiccode_17" class="indent2" block="" language="python">
                if xf_child == xb_child:
            </dt-code>
            <dt-code id="dynamiccode_18" class="indent3" block="" language="python">
                  return(recurse(xb_child,nc,sc,fseen,bseen))
            </dt-code>
            <dt-code id="dynamiccode_19" class="indent2" block="" language="python">
                # Case 4: xf and xb don't go the same way
            </dt-code>
            <dt-code id="dynamiccode_20" class="indent2" block="" language="python">
                if xf_child != xb_child:
            </dt-code>
            <dt-code id="dynamiccode_21" class="indent3" block="" language="python">
				fseen[n.feat] += 1
            </dt-code>
            <dt-code id="dynamiccode_22" class="indent3" block="" language="python">
                  posf,negf = recurse(xb_child,nc+1,sc+1,fseen,bseen)
            </dt-code>
            <dt-code id="dynamiccode_23" class="indent3" block="" language="python">
				fseen[n.feat] -= 1; bseen[n.feat] += 1
            </dt-code>
            <dt-code id="dynamiccode_24" class="indent3" block="" language="python">
                  posb,negb = recurse(xb_child,nc+1,sc  ,fseen,bseen)
            </dt-code>
            <dt-code id="dynamiccode_25" class="indent3" block="" language="python">
				bseen[n.feat] -= 1
            </dt-code>
            <dt-code id="dynamiccode_26" class="indent3" block="" language="python">
                  phi[n.feat] += posf+negb
            </dt-code>
            <dt-code id="dynamiccode_27" class="indent3" block="" language="python">
                  return((posf+posb,negf+negb))
            </dt-code>
            <dt-code id="dynamiccode_28" class="indent1" block="" language="python">
              recurse(n=T.root,0,0,[0]*len(xf),[0]*len(xf))
            </dt-code>
        </div>

  	    <div class="clear"></div>
  		</div>

    <div align=center>
    	<input type="button" class="w3-button w3-green" value="<<" onclick="ex6_reset()">
    	<input type="button" class="w3-button w3-green" value=">" onclick="dynamicStep()">
    	<input type="button" class="w3-button w3-green" value=">>" onclick="dynamicRunAll()">
    </div>

  </figure>

  <figure>

    <figcaption>
      <a href="#dynamic_ex" class="figure-number">10</a>: Dynamic programming algorithm for the tree and samples specified in <a href="#define_tree_samples" class="figure-number">5</a>.
    </figcaption>

  </figure>


    <p>
		In <a href="#dynamic_ex" class="figure-number">10</a> each node now only requires a constant amount of work.  The computational complexity to compute \(\phi_i(f,x^f,x^b)\) for all features \(i\) with this algorithm is now just:
		$$
		O(\text{\# nodes})
		$$
	</p>

	<h3 id="final_considerations">2.4 Final considerations</h3>

    <p>
		<strong>Background distribution:</strong> Finally, our original goal was to compute \(\phi_i(f,x^f)\).  In order to do so, we compute \(\phi_i(f,x^f,x^b)\) for many references \(x^b\), resulting in a run time of:
		$$
		O(|D|\times (\text{\# nodes}))
		$$
		where \(|D|\) is the number of samples in the background distribution.  In practice, using a fixed number of about 100 to 1000 references works well.
	</p>

    <p>
		<strong>Further optimization:</strong> Explaining the tree for a specific foreground and background sample requires knowing where all hybrid samples go in the tree.  Therefore we achieve the best possible complexity of \(O(\text{\# nodes})\).  However, it may be possible to compute the attributions for many background samples simultaneously in sub-linear time in order to reduce the \(O(|D|\times (\text{\# nodes}))\) cost.
	</p>

    <p>
		<strong>Ensembles of trees:</strong> Many tree based methods are ensembles (e.g., random forests, gradient boosting trees).  In order to compute that attributions for these types of models, we can simply leverage the additivity of SHAP values and explain each tree and sum the attributions for each tree.  This means that to explain a gradient boosting tree model, the computational complexity is:
		$$
		O((\text{\# trees}) \times |D|\times (\text{\# nodes}))
		$$		
	</p>

  	<hr>

    <h2 id="related_work">Related Work</h2>

    <h3 id="related_work">Methods in the SHAP package</h3>

    <p>
    	It should be noted that there are a number of alternative methods that aim to approximate SHAP values.  A few of these methods include: <i>Sampling Explainer</i>, <i>Kernel Explainer</i>, and <i>Path Dependent Tree Explainer</i>.  If you are explaining tree-based models, it may not be clear which one you should use.  In this article we briefly overview a few of these the methods and compare them to ITE.
	</p>

    <p>
      First, there are two model agnostic explanation methods in the SHAP package.  The first is <i>Sampling Explainer</i> which is an implementation of Interactions-based Method for Explanation (IME) <d-cite bibtex-key="kononenko2010efficient"></d-cite>.  This approach is based on sampling from all possible sets in order to estimate ICE SHAP values.  The second is <i>Kernel Explainer</i> which is an extension of Local Interpretable Model-agnostic Explanations (LIME) <d-cite bibtex-key="ribeiro2016should"></d-cite> to estimate ICE SHAP values.  Both <i>Sampling Explainer</i> and <i>Kernel Explainer</i> are sampling based approaches that will converge to the same SHAP values ITE obtains.  However, ITE is much faster in practice because it leverages the structure of the tree.
    </p>

    <p>
      Second, in the SHAP package there is a method named <i>Path Dependent Tree Explainer (PDTE)</i> that is meant to obtain SHAP values for tree models specifically.  <i>PDTE</i> approximates the interventional conditional expectation based on how many training samples went down paths in the tree, whereas ITE computes it exactly.  The computational complexity of <i>PDTE</i> is \(O((\text{\# leaf nodes})\times (\text{tree depth})^2)\).  In practice, <i>PDTE</i> can be faster than ITE, although it may depend on the number of references or the tree depth.  The tradeoff is that the attribution values estimated by <i>PDTE</i> are biased and do not give the ICE SHAP values.
	</p>

  <h3>A few other methods to explain trees</h3>

  <p>
    Two pre-existing methods to explain trees include <i>Gain (Gini Importance)</i> <d-cite bibtex-key="breiman_1984"></d-cite> and <i>Saabas</i> <d-cite bibtex-key="andosa_2019"></d-cite>.  Both methods consider a single ordering of the features (as opposed to all possible orderings) specified by the tree they are explaining.  Note that for an infinite ensemble of fully developed totally randomized trees <d-footnote>Where a fully developed totally randomized tree is a decision tree where each node \(n\) is partitioned using a variable \(x_i\) picked uniformly at random among those not yet used upstream of \(n\), each node \(n\) has one child for each possible value of \(x_i\), and the construction of these trees is complete only when all variables have been used.</d-footnote> and an infinitely large training sample, <i>Gain</i> is the mutual information between covariates and the outcome <d-cite bibtex-key="louppe2014understanding"></d-cite> and Saabas gives the SHAP values <d-cite bibtex-key="lundberg2020local"></d-cite>.  However, in practice trees are constructed greedily and both methods fail to satisfy the consistency axiom because they only consider a single ordering of the features.
  </p>

  <h3>Empirical evaluation</h3>

  <p>
		In this article we do not empirically evaluate these methods, because evaluating interpretability methods can difficult due to the subjective nature of an explanation.  Instead, we identify a way of assigning credit that comes equipped with a set of desirable properties (i.e., Shapley values) and show how to compute them exactly for trees with a tractable algorithm.  This means that the credit allocation we obtain comes built in with many of the desirable properties that ablation tests aim to measure (e.g., consistency).  That being said, <d-cite bibtex-key="lundberg2020local"></d-cite> provides an in-depth empirical comparison of many of these methods using a variety of ablation tests.
	</p>	

    <h2 id="references">Acknowledgements</h2>

    <p>This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-1762114.  Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors(s) and do not necessarily reflect the views of the National Science Foundation.</p>

	</div>

  </div>

</div>


<!-- Figure 1 - Kaggle methods d3 bar plot -->
<script src="js/kaggleMethods.js"></script>

<!-- Example 1 - Computing Shapley values -->
<script src="js/shapleyValues.js"></script>

<!-- Example 2 - Computing Shapley values CES vs RBS -->
<script src="js/linearModelCESvsRBS.js"></script>

<!-- Example 3 - Initial tree parameters -->
<script src="js/treeData0.js"></script>

<!-- Example 3 - Creating tree and samples -->
<script src="js/createTreeSamples.js"></script>

<!-- Example 4 - Brute force algorithm -->
<script src="js/brute_force_ex.js"></script>

<!-- Example 5 - Naive tree algorithm -->
<script src="js/naive_ex.js"></script>

<!-- Example 6 - DP tree algorithm -->
<script src="js/dp_ex.js"></script>

<script>
function hideshow(type) {
    var x = document.getElementById(type);
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>

<d-appendix>
  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="bibliography.bib"></d-bibliography>

<script src="https://distill.pub/template.v2.js"></script>

<script>
  window.onload = function() {
    // Set indentations for code
    for (var i=0; i < 10; i++) {
      var a = document.getElementById("bf_"+(i+1));
      var b = a.getElementsByTagName("pre")[0];
      b.style.marginTop = "0px";
      b.style.marginBottom = "0px";
      b.style.paddingTop = "0px";
      b.style.paddingBottom = "0px";
      b.style.fontSize = "14px";
      b.style.background = "#ffffff";
      if (i > 0) {
        b.style.textIndent = "18px";
      }
      if (i > 2) {
        b.style.textIndent = "36px";
      }
      if (i > 3) {
        b.style.textIndent = "54px";
      } 
    }

    // Set indentations for code
    for (var i=0; i < 27; i++) {
      var a = document.getElementById("naivecode_"+(i+1));
      var b = a.getElementsByTagName("pre")[0];
      b.style.marginTop = "0px";
      b.style.marginBottom = "0px";
      b.style.paddingTop = "0px";
      b.style.paddingBottom = "0px";
      b.style.fontSize = "14px";
      b.style.background = "#ffffff";
      if (a.classList[0] == "indent0") {
        b.style.textIndent = "0px";
      } else if (a.classList[0] == "indent1") {
        b.style.textIndent = "18px";
      } else if (a.classList[0] == "indent2") {
        b.style.textIndent = "36px";
      } else if (a.classList[0] == "indent3") {
        b.style.textIndent = "54px";
      } else if (a.classList[0] == "indent4") {
        b.style.textIndent = "72px";
      } else if (a.classList[0] == "indent5") {
        b.style.textIndent = "90px";
      }
    }

    // Set indentations for code
    for (var i=0; i < 28; i++) {
      var a = document.getElementById("dynamiccode_"+(i+1));
      var b = a.getElementsByTagName("pre")[0];
      b.style.marginTop = "0px";
      b.style.marginBottom = "0px";
      b.style.paddingTop = "0px";
      b.style.paddingBottom = "0px";
      b.style.fontSize = "14px";
      b.style.background = "#ffffff";
      if (a.classList[0] == "indent0") {
        b.style.textIndent = "0px";
      } else if (a.classList[0] == "indent1") {
        b.style.textIndent = "18px";
      } else if (a.classList[0] == "indent2") {
        b.style.textIndent = "36px";
      } else if (a.classList[0] == "indent3") {
        b.style.textIndent = "54px";
      } else if (a.classList[0] == "indent4") {
        b.style.textIndent = "72px";
      } else if (a.classList[0] == "indent5") {
        b.style.textIndent = "90px";
      }
    }

  };
</script>


</body>
</html>
